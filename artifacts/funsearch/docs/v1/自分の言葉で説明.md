# 従来手法との違い、新しさ

* funsearch の手法を Symbolic Regression に適用している (従来手法はサブツリーミューテーションなど)
    * 遺伝的アルゴリズムを使いつつ、係数はオプティマイザで同定して、LLMを用いて変異させる、という全体の流れが新しい
    * 機械学習を使った手法としてNeSymReSがあるけど、遺伝的アルゴリズムを行っていない
    * 遺伝的アルゴリズムを行う従来手法(PySRなど)は係数だけオプティマイザで同定することはしてない
    * funsearch は数式を対象としていないし係数同定でオプティマイザとかやってないしスコア比較で勝ってる
    * LMXみてれない
* 性能評価をくわしく行っている
    * PPL を評価している
    * ノイズとか評価している
    
## LMXとの比較
```
LMX (Meyerson et al., 2023) has included symbolic regression as one of its experimental tasks,
though primarily as a proof-of-concept rather than aiming to achieve state-of-the-art performance.
Their implementation does not incorporate domain-specific prior knowledge, generates complete
equations with LLM instead of optimizable skeletons, uses older LLM models (Galactica, Pythia),
and lacks several design elements present in LLM-SR such as equation-as-program representation
and multi-island dynamic memory management for diverse exploration
```

# 使用されている論理的アプローチ

* 進化的アルゴリズム、島アルゴリズム
* スコアのボルツマン分布に従って変異用サンプルを選択
* オッカムの剃刀、MDLの原則でクラスタから関数を選択
* LLMで変異
* オプティマイザで係数同定
* PPLというのをつかっためっちゃ詳しい評価をしてる
    * モデルが知らない結果を発見できることを示そうとしている
    * 関数探索のフレームワークとして数学的に有用であることを示そうとしている
    
## PPLについて
ollama とか使ってると確率分布もセットで返してくれるので、以下のような計算ができる

```
正確に数式的に表現すると、以下のようになります。

1.  **文脈 (Context) を C とする。**
2.  **正解方程式のトークン列を t₁, t₂, t₃, ..., t<0xE2><0x82><0x99> とする。**
3.  **各ステップで以下の条件付き確率を計算する:**
    *   P(t₁ | C) ：文脈Cのもとで、最初のトークンt₁が出現する確率
    *   P(t₂ | C, t₁) ：文脈Cとトークンt₁のもとで、2番目のトークンt₂が出現する確率
    *   P(t₃ | C, t₁, t₂) ：文脈Cとトークンt₁, t₂のもとで、3番目のトークンt₃が出現する確率
    *   ...
    *   P(t<0xE2><0x82><0x99> | C, t₁, ..., t<0xE2><0x82><0x99>₋₁) ：それまでの文脈とトークン列のもとで、最後のトークンt<0xE2><0x82><0x99>が出現する確率
4.  **それぞれの確率の対数を取る:**
    *   log P(t₁ | C)
    *   log P(t₂ | C, t₁)
    *   log P(t₃ | C, t₁, t₂)
    *   ...
    *   log P(t<0xE2><0x82><0x99> | C, t₁, ..., t<0xE2><0x82><0x99>₋₁)
5.  **対数確率の合計を計算する:**
    *   Sum = Σᵢ<0xE1><0xB5><0xA3>₁<0xE1><0xB5><0xA3> log P(tᵢ | C, t₁, ..., tᵢ₋₁)
6.  **合計をトークン数 N で割る（対数確率の平均を計算する）:**
    *   AvgLogProb = Sum / N

**Perplexity (PPL) は、この「対数確率の平均」にマイナスをつけて指数関数 (exp) を取ったものです。**

*   **PPL = exp(- AvgLogProb) = exp { - (1/N) * Σᵢ<0xE1><0xB5><0xA3>₁<0xE1><0xB5><0xA3> log P(tᵢ | C, t₁, ..., tᵢ₋₁) }**

「それぞれの確率の対数とってNでわる感じか」というのは、`AvgLogProb` （対数確率の平均）を計算する部分です。Perplexityはその値を使って最終的に計算される指標となります。
```

# 自分が研究を進めている分野への適用可能性
以下の三つの適用方法を考えた

* 周期分極反転ドメイン幅配列を関数として表現し、広帯域なスペクトル分布になる関数をLLM-SRで同定する
* 関数としてではなく、indexに対するドメイン幅をアルゴリズムによって決定する (フラクタル等) ことを考えると funsearch のようにプログラムの同定に帰着できそう
* 一定周期分極反転構造の場合はかんたんな計算方法があってevaluateが楽、この制約下でも、量子井戸インターミキシングとかいう技術でセルマイヤーの分散式によって得られるような位相不整合の波長依存性をコントロールすることができるらしい、ここの依存性がどうなっている時にスペクトル分布が広帯域になるかについて、LLM-SRで同定すればマテリアルインフォマティクス？

## 適用可能性を調べるための予備実験

`論文の検証/pilot_study.ipynb` で検証を兼ねた予備実験を行った

非線形結合モード方程式は特殊な条件下で解析的に近似解を求めることができるため、その関数をLLM-SRで同定できるか試した

既存知識にないことを保証するために、MgO:SLTの分散式を含む方程式を探索した

さすがに難しすぎてる気がしたけど、論文のPoCコードを編集してgemini3:12bで2日ぐらい放置して6000回探索したところ、ピーク位置と幅をそれなりに近似している謎の関数が発見できた (この時は自作のコードではなくLLM-SRのコードを使用してる)

自作コードで現在、[ollamaの不具合](https://github.com/ollama/ollama/issues/10040)によりgemini3:12bが使えておらず、qwen2.5-coderを使用している (元のコードはシングルスレッドだしsleep(5)があったから止まらなかったのだと思う)

gemini3:12bはもとからshgやNPDAについて知識があるが、qwen2.5-coderにはshgやNPDAの知識が全くない、セルマイヤーの分散式の知識も全くない

LLM がある程度知識持ってるおかげで探索が速いとすれば、qwen2.5-coderでは全く見つからない可能性もあると思ったけど、以外といいスコア出現してきてる

evaluator の問題直したらほぼ正解の関数を603回でqwenが発見できた

## スペクトル分布の広帯域化のための分極反転ドメイン幅構造の探索

自身の研究の本題であるスペクトル分布を広帯域にする分極反転ドメイン幅関数および構造決定アルゴリズムの探索でのevaluateでは、事前にデータセットを用意することが不可能

実際に提示された関数またはアルゴリズムで定義される構造を使ってスペクトル分布の計算を行い、計算された帯域を何らかの基準で定量的に評価し、それをevaluateの結果とする必要がある

## 屈折率分散の設計

周期分極反転構造なら変換効率はすぐ計算できる、ここで量子井戸構造等を使って$n(\lambda)$を変化させることができるならば、どのように材料設計すれば広帯域化ができるか探索できそう

量子井戸インターミキシングという手法らしい
