### **A Generalized Mathematical Formulation of Conceptual Search Framework**

#### **1. The Search Apparatus**

A Search Apparatus is a tuple $\mathcal{A} := (\mathcal{X}, \mathbb{S}, \mathcal{C}, \Lambda, \Psi)$.

*   $\mathcal{X}$: A set, called the **Model Description Space**.
*   $\mathbb{S}$: A family of subsets of $\mathcal{X}$, i.e., $\mathbb{S} \subseteq \mathcal{P}(\mathcal{X})$. The elements $S \in \mathbb{S}$ are called **Search Units**.
*   $\mathcal{C}$: The **Criteria Space**, defined as $\mathcal{C} := \mathbb{R}_{\ge 0} \times C_\Psi$ for some set $C_\Psi$.
*   $\Lambda: \mathcal{X} \to \mathbb{R}_{\ge 0}$: A functional.
*   $\Psi: \mathbb{S} \to C_\Psi$: A map.

The framework induces a value function $V: \mathbb{S} \to \mathcal{C}$ defined for any non-empty $S \in \mathbb{S}$ by
$$
V(S) := \left( \inf_{x \in S} \Lambda(x), \; \Psi(S) \right).
$$

#### **2. Search and Solution**

A search process is defined by a generator $G: \mathcal{P}(\mathbb{S} \times \mathcal{C}) \to \mathcal{P}_{fin}(\mathbb{S})$ that satisfies $G(\mathcal{E}) \cap \pi_1(\mathcal{E}) = \emptyset$ for any $\mathcal{E} \subseteq \mathbb{S} \times \mathcal{C}$. The process generates a sequence of archives $\{\mathcal{E}_k\}_{k \in \mathbb{N}}$:

1.  $\mathcal{E}_0 := \emptyset$.
2.  $\mathcal{E}_{k+1} := \mathcal{E}_k \cup \{ (S, V(S)) \mid S \in G(\mathcal{E}_k) \}$.

We consider only **terminating** search processes, for which there exists a $K \in \mathbb{N}$ such that $G(\mathcal{E}_K) = \emptyset$. Let $K_{min}$ be the smallest such integer. The **final archive** is
$$
\mathcal{E}_{final} := \mathcal{E}_{K_{min}}.
$$

Let $(\preceq)$ be a given partial order on $\mathcal{C}$, and let $(\prec)$ be its strict part.

*   The **Set of Minimal Criteria** is
    $$
    \mathcal{C}^* := \{ c \in \pi_2(\mathcal{E}_{final}) \mid \nexists c' \in \pi_2(\mathcal{E}_{final}) \text{ s.t. } c' \prec c \}.
    $$
*   The **Solution Set** is
    $$
    \mathbb{S}^* := \{ S \in \pi_1(\mathcal{E}_{final}) \mid V(S) \in \mathcal{C}^* \}.
    $$

#### **3. Parallelization (An Abstract Formulation)**

1.  Define a set of computational agents $\{P_i\}_{i \in I}$ and a Shared State Space $\mathcal{M}$.

2.  Define a communication/synchronization mechanism, which may include:
    *   A communication topology between the agents.
    *   A protocol for synchronized access to the shared state $\mathcal{M}$.

3.  The behavior of the overall system, $P_{sys} := \underset{i \in I}{\parallel} P_i$, is described as an interleaving of events, including internal computations, message passing, and shared state modifications.

### **An Instance: Optimization over a Parameterized Family of Manifolds**

We specify the Conceptual Search Framework for optimizing over a family of manifolds indexed by a parameter space.

#### **1. Formulation**

Let the **Model Description Space** $\mathcal{X}$ be a Hilbert space $\mathcal{H}$. Let $\Theta$ be a set, the **Parameter Space**. For each $\theta \in \Theta$, let $\Phi_\theta: U \to \mathcal{H}$ be a differentiable immersion from a domain $U \subseteq \mathbb{R}^m$, defining an $m$-dimensional manifold $M(\theta) := \Phi_\theta(U)$.

The Search Apparatus $\mathcal{A} = (\mathcal{H}, \mathbb{M}, \mathcal{C}, L, J)$ is:

*   $\mathcal{H}$: A Hilbert space.
*   $\mathbb{M}$: The family of immersed submanifolds, $\mathbb{M} := \{ M(\theta) \mid \theta \in \Theta \}$.
*   $L: \mathcal{H} \to \mathbb{R}_{\ge 0}$: A functional, defined below.
*   $J: \Theta \to C_J$: A map assigning an attribute to each manifold via its parameter.

The **Criteria Space** is $\mathcal{C} := \mathbb{R}_{\ge 0} \times C_J$. The **Value Function** $V: \mathbb{M} \to \mathcal{C}$ is:
$$
V(M(\theta)) := \left( \inf_{f \in M(\theta)} L(f), \; J(\theta) \right).
$$
The search objective is to find the set of Pareto-optimal parameters $\Theta^* \subseteq \Theta$. In the case where $m=0$, each $M(\theta)$ is a singleton $\{f_\theta\}$, and the infimum reduces to $L(f_\theta)$.

#### **2. The Functional $L$ and Gradient-Based Search**

Let $\mathcal{Y}$ be a Hilbert space containing an observation $\mathbf{z}$. The functional $L$ is given by
$$
L(f) := \| \mathcal{O}(f) - \mathbf{z} \|_{\mathcal{Y}}^2 + \Omega(f),
$$
where the observation operator $\mathcal{O}: \mathcal{H} \to \mathcal{Y}$ and regularizer $\Omega: \mathcal{H} \to \mathbb{R}_{\ge 0}$ are Fréchet differentiable, rendering $L$ Fréchet differentiable. This enables a bi-level optimization:

1.  **Inner Optimization:** For a fixed $\theta$, the problem $\inf_{f \in M(\theta)} L(f)$ is equivalent to $\min_{u \in U} L(\Phi_\theta(u))$. As $L$ and $\Phi_\theta$ are differentiable with respect to $u$, this is a standard gradient-based optimization problem on $\mathbb{R}^m$. This procedure is an implementation of Riemannian gradient descent on $M(\theta)$.

2.  **Outer Optimization:** For a gradient-based search over $\mathbb{M}$, we now posit that **(i)** the parameter space $\Theta$ is a differentiable manifold, and **(ii)** the map $(\theta, u) \mapsto \Phi_\theta(u)$ is differentiable. Under these assumptions and further regularity conditions, the Envelope Theorem ensures the optimal value function $L^*(\theta) := \inf_{f \in M(\theta)} L(f)$ is differentiable with respect to $\theta$. This permits a gradient-based search on the manifold $\Theta$, generalizing methods like Variable Projection to arbitrary manifold-structured parameter spaces.

# Ideas
* オプティマイザは鉄板のBFGSから初めてLBFGSやらRAdamやらいろいろ試すと良いかも、あとVarPro
* マルチスタートしてから分散を計算し$C_J$に保持
* NSGA-IIが良いらしい
* LLMと通常のGPハイブリッドしても良いかも、てかGを沢山の種類用意してアンサンブルできるかも
* L1正則化すると良いらしい
* 感覚的な話だが、LLMに複数の提案をいっぺんにしてもらうべきだと痛感している

# TODO
* VとGの具象と並列化部分の具象
* そういえば図示の時に変数ごとに分布と言っていたが、モデルを使って分布を書くことは可能でもデータセットが対応しない場合がある。とりあえず1次元の時だけ分布を用意すればよいか聞く。
