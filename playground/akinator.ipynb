{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. エントロピー (Entropy)\n",
    "\n",
    "エントロピーは、与えられた確率分布の不 certaintyを測定します。定義は以下の通りです：\n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_{i=1}^{n} P(x_i) \\log_2 P(x_i)\n",
    "$$\n",
    "\n",
    "ここで、$ X $ は離散的な確率変数で、$ x_i $ はその取り得る値の一つです。\n",
    "\n",
    "### 2. 初期確率 (Initial Probabilities)\n",
    "\n",
    "初期確率は、すべてのケースが等しい確率で起こると仮定しています。例えば、$ n $ 個のケースがある場合：\n",
    "\n",
    "$$\n",
    "P(\\text{case}_i) = \\frac{1}{n} \\quad \\text{for all } i\n",
    "$$\n",
    "\n",
    "### 3. 選択肢の確率 (Choice Probabilities)\n",
    "\n",
    "選択肢の確率は、各ケースに対する条件付き確率と初期確率を用いて計算されます。例えば、選択肢 $ c_j $ の確率は：\n",
    "\n",
    "$$\n",
    "P(c_j) = \\sum_{i=1}^{n} P(c_j | \\text{case}_i) \\cdot P(\\text{case}_i)\n",
    "$$\n",
    "\n",
    "### 4. 事後確率 (Posterior Probabilities)\n",
    "\n",
    "選択肢 $ c_j $ が与えられた場合の、各ケースの事後確率は：\n",
    "\n",
    "$$\n",
    "P(\\text{case}_i | c_j) = \\frac{P(c_j | \\text{case}_i) \\cdot P(\\text{case}_i)}{P(c_j)}\n",
    "$$\n",
    "\n",
    "### 5. 事後エントロピー (Conditional Entropy)\n",
    "\n",
    "選択肢 $ c_j $ に対する事後エントロピーは：\n",
    "\n",
    "$$\n",
    "H(X | c_j) = -\\sum_{i=1}^{n} P(\\text{case}_i | c_j) \\log_2 P(\\text{case}_i | c_j)\n",
    "$$\n",
    "\n",
    "### 6. 全体の事後エントロピー (Total Conditional Entropy)\n",
    "\n",
    "全体の事後エントロピーは、各選択肢に対する事後エントロピーの期待値です：\n",
    "\n",
    "$$\n",
    "H(X | C) = \\sum_{j} P(c_j) \\cdot H(X | c_j)\n",
    "$$\n",
    "\n",
    "### 7. 情報利得 (Information Gain)\n",
    "\n",
    "情報利得は、初期エントロピーから事後エントロピーを引いた値です：\n",
    "\n",
    "$$\n",
    "\\text{IG}(C; X) = H(X) - H(X | C)\n",
    "$$\n",
    "\n",
    "### 具体的な計算手順\n",
    "\n",
    "1. **初期確率の計算**：\n",
    "   $$\n",
    "   P(\\text{case}_i) = \\frac{1}{n} \\quad \\text{for all } i\n",
    "   $$\n",
    "   \n",
    "2. **選択肢の確率の計算**：\n",
    "   $$\n",
    "   P(c_j) = \\sum_{i=1}^{n} P(c_j | \\text{case}_i) \\cdot P(\\text{case}_i)\n",
    "   $$\n",
    "\n",
    "3. **事後確率の計算**：\n",
    "   $$\n",
    "   P(\\text{case}_i | c_j) = \\frac{P(c_j | \\text{case}_i) \\cdot P(\\text{case}_i)}{P(c_j)}\n",
    "   $$\n",
    "\n",
    "4. **各選択肢に対する事後エントロピーの計算**：\n",
    "   $$\n",
    "   H(X | c_j) = -\\sum_{i=1}^{n} P(\\text{case}_i | c_j) \\log_2 P(\\text{case}_i | c_j)\n",
    "   $$\n",
    "\n",
    "5. **全体の事後エントロピーの計算**：\n",
    "   $$\n",
    "   H(X | C) = \\sum_{j} P(c_j) \\cdot H(X | c_j)\n",
    "   $$\n",
    "\n",
    "6. **情報利得の計算**：\n",
    "   $$\n",
    "   \\text{IG}(C; X) = H(X) - H(X | C)\n",
    "   $$\n",
    "\n",
    "これらの式を用いて、コードは情報利得を計算しています。具体的には、初期確率に基づくエントロピーと、選択肢に対する条件付き確率から導出される事後エントロピーを計算し、その差を情報利得としています。\n",
    "\n",
    "### 例示\n",
    "\n",
    "例えば、3つのケース（りんご、バナナ、スイカ）と5つの選択肢（はい、多少はい、わからない、多少いいえ、いいえ）がある場合、以下のような計算を行います。\n",
    "質問は、たとえば\"赤いですか？\"だったとします。\n",
    "\n",
    "1. **初期確率**：\n",
    "   $$\n",
    "   P(\\text{りんご}) = P(\\text{バナナ}) = P(\\text{スイカ}) = \\frac{1}{3}\n",
    "   $$\n",
    "\n",
    "2. **選択肢の確率**：\n",
    "   例えば、選択肢 \"はい\" の確率は：\n",
    "   $$\n",
    "   P(\\text{はい}) = P(\\text{はい} | \\text{りんご}) \\cdot P(\\text{りんご}) + P(\\text{はい} | \\text{バナナ}) \\cdot P(\\text{バナナ}) + P(\\text{はい} | \\text{スイカ}) \\cdot P(\\text{スイカ})\n",
    "   $$\n",
    "\n",
    "3. **事後確率**：\n",
    "   例えば、選択肢 \"はい\" が与えられたときのりんごの事後確率は：\n",
    "   $$\n",
    "   P(\\text{りんご} | \\text{はい}) = \\frac{P(\\text{はい} | \\text{りんご}) \\cdot P(\\text{りんご})}{P(\\text{はい})}\n",
    "   $$\n",
    "\n",
    "4. **事後エントロピー**：\n",
    "   例えば、選択肢 \"はい\" に対する事後エントロピーは：\n",
    "   $$\n",
    "   H(X | \\text{はい}) = -\\sum_{i \\in \\{\\text{りんご}, \\text{バナナ}, \\text{スイカ}\\}} P(\\text{case}_i | \\text{はい}) \\log_2 P(\\text{case}_i | \\text{はい})\n",
    "   $$\n",
    "\n",
    "5. **全体の事後エントロピー**：\n",
    "   $$\n",
    "   H(X | C) = \\sum_{j \\in \\{\\text{はい}, \\text{多少はい}, \\text{わからない}, \\text{多少いいえ}, \\text{いいえ}\\}} P(c_j) \\cdot H(X | c_j)\n",
    "   $$\n",
    "\n",
    "6. **情報利得**：\n",
    "   $$\n",
    "   \\text{IG}(C; X) = H(X) - H(X | C)\n",
    "   $$\n",
    "   \n",
    "   ここで、$ H(X) $は初期エントロピーで、等確率の場合：\n",
    "   $$\n",
    "   H(X) = -\\sum_{i=1}^{3} \\frac{1}{3} \\log_2 \\frac{1}{3} = \\log_2 3\n",
    "   $$\n",
    "\n",
    "これらの計算を通じて、情報利得が導出されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain: 0.4626097187381275\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "CHOICES = [\"はい\", \"多分はい\", \"わからない\", \"多分いいえ\", \"いいえ\"]\n",
    "\n",
    "\n",
    "def calculate_entropy(probs):\n",
    "    \"\"\"Calculate the entropy of a list of probabilities.\"\"\"\n",
    "    return -sum(p * math.log2(p) for p in probs.values() if p > 0)\n",
    "\n",
    "\n",
    "def initial_probabilities(cases):\n",
    "    \"\"\"Calculate the prior probabilities assuming uniform distribution.\"\"\"\n",
    "    num_cases = len(cases)\n",
    "    prob = 1 / num_cases\n",
    "    return {case: prob for case in cases}\n",
    "\n",
    "\n",
    "def calculate_choice_probs(conditional_choice_probs, initial_case_probs):\n",
    "    \"\"\"Calculate the probabilities for each choice.\"\"\"\n",
    "    choice_probs = {choice: 0 for choice in CHOICES}\n",
    "\n",
    "    for case in conditional_choice_probs:\n",
    "        for choice, probs in conditional_choice_probs[case].items():\n",
    "            choice_probs[choice] += probs * initial_case_probs[case]\n",
    "\n",
    "    return choice_probs\n",
    "\n",
    "\n",
    "def calculate_posterior_entropy(conditional_choice_probs, initial_case_probs, choice_probs):\n",
    "    \"\"\"Calculate the weighted sum of entropies for each choice.\"\"\"\n",
    "    H_posterior = 0\n",
    "\n",
    "    for choice in CHOICES:\n",
    "        if choice_probs[choice] == 0:\n",
    "            continue  # Skip if this choice has zero probability\n",
    "\n",
    "        posterior_probs = {}\n",
    "        for case in conditional_choice_probs.keys():\n",
    "            cond_prob = conditional_choice_probs[case].get(choice, 0)\n",
    "            prior_prob = initial_case_probs[case]\n",
    "            posterior_prob = (cond_prob * prior_prob) / choice_probs[choice]\n",
    "            posterior_probs[case] = posterior_prob\n",
    "\n",
    "        # Calculate entropy for this choice's posterior distribution\n",
    "        ent = calculate_entropy(posterior_probs)\n",
    "        H_posterior += ent * choice_probs[choice]\n",
    "\n",
    "    return H_posterior\n",
    "\n",
    "\n",
    "def calculate_information_gain(question_data):\n",
    "    \"\"\"Calculate the information gain for a given question and probability data.\"\"\"\n",
    "    cases = list(question_data['probs'].keys())\n",
    "    initial_case_probs = initial_probabilities(cases)\n",
    "\n",
    "    H_initial = calculate_entropy(initial_case_probs)\n",
    "\n",
    "    choice_probs = calculate_choice_probs(\n",
    "        question_data['probs'], initial_case_probs)\n",
    "\n",
    "    H_posterior = calculate_posterior_entropy(\n",
    "        question_data['probs'], initial_case_probs, choice_probs)\n",
    "\n",
    "    information_gain = H_initial - H_posterior\n",
    "    return information_gain\n",
    "\n",
    "\n",
    "# Define the question and probabilities using dictionaries for clarity\n",
    "question_data = {\n",
    "    'question': '赤いですか？',\n",
    "    'probs': {\n",
    "        'りんご': {'はい': 0.7, '多分はい': 0.2, 'わからない': 0.05, '多分いいえ': 0.03, 'いいえ': 0.02},\n",
    "        'ばなな': {'はい': 0.0, '多分はい': 0.1, 'わからない': 0.2, '多分いいえ': 0.3, 'いいえ': 0.4},\n",
    "        'スイカ': {'はい': 0.5, '多分はい': 0.1, 'わからない': 0.2, '多分いいえ': 0.1, 'いいえ': 0.1}\n",
    "    }\n",
    "}\n",
    "\n",
    "question_data2 = {\n",
    "    'question': '種の色は黒ですか？',\n",
    "    'probs': {\n",
    "        'りんご': {'はい': 0.7, '多分はい': 0.2, 'わからない': 0.05, '多分いいえ': 0.03, 'いいえ': 0.02},\n",
    "        'スイカ': {'はい': 0.8, '多分はい': 0.1, 'わからない': 0.05, '多分いいえ': 0.05, 'いいえ': 0.0}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Calculate and print the information gain\n",
    "info_gain = calculate_information_gain(question_data)\n",
    "print(f\"Information Gain: {info_gain}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
